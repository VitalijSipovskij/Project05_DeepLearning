{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nRybZWRW0f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "9c652805-86ec-411a-acc9-c67b130b5bb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Age  Gender        EducationLevel  HouseholdIncome  \\\n",
              "0     25  Female     Bachelor's Degree        1500000.0   \n",
              "1     48  Female     Bachelor's Degree         800000.0   \n",
              "2     49    Male       Master's Degree       30000000.0   \n",
              "3     38    Male     Bachelor's Degree          20000.0   \n",
              "4     45    Male     Bachelor's Degree        5000000.0   \n",
              "..   ...     ...                   ...              ...   \n",
              "102   38  Female       Master's Degree         400000.0   \n",
              "103   25    Male     Bachelor's Degree         840000.0   \n",
              "104   46    Male       Master's Degree       10000000.0   \n",
              "105   58  Female  High School Graduate       11200000.0   \n",
              "106   24  Female     Bachelor's Degree         456000.0   \n",
              "\n",
              "     Frequency_LaptopDesktop  Frequency_Smartphone  Frequency_Tablet  \\\n",
              "0                          4                     5                 0   \n",
              "1                          3                     5                 5   \n",
              "2                          4                     4                 0   \n",
              "3                          1                     0                 1   \n",
              "4                          5                     5                 3   \n",
              "..                       ...                   ...               ...   \n",
              "102                        5                     5                 3   \n",
              "103                        4                     5                 3   \n",
              "104                        4                     5                 3   \n",
              "105                        5                     5                 0   \n",
              "106                        1                     5                 0   \n",
              "\n",
              "     Frequency_SmartHomeDevices  Frequency_WearableDevices  \\\n",
              "0                             4                          0   \n",
              "1                             4                          5   \n",
              "2                             2                          0   \n",
              "3                             1                          1   \n",
              "4                             5                          5   \n",
              "..                          ...                        ...   \n",
              "102                           3                          4   \n",
              "103                           0                          2   \n",
              "104                           5                          0   \n",
              "105                           0                          5   \n",
              "106                           3                          0   \n",
              "\n",
              "     Frequency_GamingConsoles  ...  Influence_PromoActivities  \\\n",
              "0                           0  ...                          3   \n",
              "1                           2  ...                          4   \n",
              "2                           0  ...                          3   \n",
              "3                           2  ...                          2   \n",
              "4                           0  ...                          3   \n",
              "..                        ...  ...                        ...   \n",
              "102                         3  ...                          3   \n",
              "103                         0  ...                          3   \n",
              "104                         0  ...                          5   \n",
              "105                         0  ...                          1   \n",
              "106                         0  ...                          3   \n",
              "\n",
              "     Influence_SocialPopularity  Influence_ExpertRecommendation  \\\n",
              "0                             5                               4   \n",
              "1                             4                               4   \n",
              "2                             3                               3   \n",
              "3                             3                               3   \n",
              "4                             3                               3   \n",
              "..                          ...                             ...   \n",
              "102                           4                               3   \n",
              "103                           3                               4   \n",
              "104                           4                               1   \n",
              "105                           1                               1   \n",
              "106                           3                               3   \n",
              "\n",
              "     Influence_Endorsements  Barrier_HighPrice  Barrier_TechKnowledge  \\\n",
              "0                         2                  3                      4   \n",
              "1                         2                  2                      4   \n",
              "2                         1                  5                      5   \n",
              "3                         2                  1                      3   \n",
              "4                         3                  3                      3   \n",
              "..                      ...                ...                    ...   \n",
              "102                       3                  2                      1   \n",
              "103                       2                  3                      1   \n",
              "104                       1                  4                      2   \n",
              "105                       1                  3                      2   \n",
              "106                       2                  4                      3   \n",
              "\n",
              "     Barrier_PrivacySecurity  Barrier_ReliableInternet  \\\n",
              "0                          4                         3   \n",
              "1                          4                         4   \n",
              "2                          5                         3   \n",
              "3                          2                         3   \n",
              "4                          3                         3   \n",
              "..                       ...                       ...   \n",
              "102                        2                         2   \n",
              "103                        2                         1   \n",
              "104                        5                         2   \n",
              "105                        4                         1   \n",
              "106                        4                         3   \n",
              "\n",
              "     Barrier_ChangeReluctance  LikelinessToRecommend  \n",
              "0                           4                      4  \n",
              "1                           4                      5  \n",
              "2                           4                      3  \n",
              "3                           2                      3  \n",
              "4                           3                      5  \n",
              "..                        ...                    ...  \n",
              "102                         2                      2  \n",
              "103                         4                      2  \n",
              "104                         1                      3  \n",
              "105                         1                      5  \n",
              "106                         3                      3  \n",
              "\n",
              "[107 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-515a14a8-72c8-4614-beec-08464f0c01c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>EducationLevel</th>\n",
              "      <th>HouseholdIncome</th>\n",
              "      <th>Frequency_LaptopDesktop</th>\n",
              "      <th>Frequency_Smartphone</th>\n",
              "      <th>Frequency_Tablet</th>\n",
              "      <th>Frequency_SmartHomeDevices</th>\n",
              "      <th>Frequency_WearableDevices</th>\n",
              "      <th>Frequency_GamingConsoles</th>\n",
              "      <th>...</th>\n",
              "      <th>Influence_PromoActivities</th>\n",
              "      <th>Influence_SocialPopularity</th>\n",
              "      <th>Influence_ExpertRecommendation</th>\n",
              "      <th>Influence_Endorsements</th>\n",
              "      <th>Barrier_HighPrice</th>\n",
              "      <th>Barrier_TechKnowledge</th>\n",
              "      <th>Barrier_PrivacySecurity</th>\n",
              "      <th>Barrier_ReliableInternet</th>\n",
              "      <th>Barrier_ChangeReluctance</th>\n",
              "      <th>LikelinessToRecommend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>Female</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>1500000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>Female</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>800000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49</td>\n",
              "      <td>Male</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>30000000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38</td>\n",
              "      <td>Male</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>5000000.0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>38</td>\n",
              "      <td>Female</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>400000.0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>25</td>\n",
              "      <td>Male</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>840000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>46</td>\n",
              "      <td>Male</td>\n",
              "      <td>Master's Degree</td>\n",
              "      <td>10000000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>58</td>\n",
              "      <td>Female</td>\n",
              "      <td>High School Graduate</td>\n",
              "      <td>11200000.0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>24</td>\n",
              "      <td>Female</td>\n",
              "      <td>Bachelor's Degree</td>\n",
              "      <td>456000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-515a14a8-72c8-4614-beec-08464f0c01c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-515a14a8-72c8-4614-beec-08464f0c01c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-515a14a8-72c8-4614-beec-08464f0c01c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d6ee7b92-84a9-4f60-b6e5-d8d6d1726f50\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6ee7b92-84a9-4f60-b6e5-d8d6d1726f50')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d6ee7b92-84a9-4f60-b6e5-d8d6d1726f50 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"Dataset - Factors Influencing Technology Adoption in Consumer Households.csv\")\n",
        "\n",
        "# Preprocessing features from dataset\n",
        "selected_features = [\n",
        "    'Age', 'HouseholdIncome', 'Frequency_LaptopDesktop',\n",
        "    'Frequency_Smartphone', 'Frequency_Tablet', 'Frequency_SmartHomeDevices'\n",
        "]\n",
        "\n",
        "# Polynomial Multiple Regression Task\n",
        "X_poly = data[selected_features].values\n",
        "y_poly = data['LikelinessToRecommend'].values  # Target variable for regression\n",
        "\n",
        "data['LikelinessToAdopt'] = data['LikelinessToAdopt'].apply(lambda x: 1 if x >= 3 else 0)\n",
        "X_class = data[selected_features].values\n",
        "y_class = data['LikelinessToAdopt'].values  # Target variable for classification\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly, y_poly, test_size=0.2, random_state=42)\n",
        "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_poly_train_scaled = scaler.fit_transform(X_poly_train)\n",
        "X_poly_test_scaled = scaler.transform(X_poly_test)\n",
        "\n",
        "X_class_train_scaled = scaler.fit_transform(X_class_train)\n",
        "X_class_test_scaled = scaler.transform(X_class_test)\n",
        "\n",
        "# Display data\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the model\n",
        "model_tf = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_poly_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_tf.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "history_tf = model_tf.fit(X_poly_train_scaled, y_poly_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_tf, _ = model_tf.evaluate(X_poly_test_scaled, y_poly_test)\n",
        "print(\"Mean Squared Error (TensorFlow/Keras):\", mse_tf)\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "# Implement classification\n",
        "# Define the model for classification for TenserFlow\n",
        "model_tf_classification = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_class_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_tf_classification.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_tf_classification = model_tf_classification.fit(X_class_train_scaled, y_class_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss_tf_classification, accuracy_tf_classification = model_tf_classification.evaluate(X_class_test_scaled, y_class_test)\n",
        "print(\"Test Loss (TensorFlow/Keras):\", loss_tf_classification)\n",
        "print(\"Test Accuracy (TensorFlow/Keras):\", accuracy_tf_classification)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqknbTPz0rOe",
        "outputId": "405b40af-4783-4d1b-a69c-9875716b031a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - 2s 231ms/step - loss: 17.6455 - mse: 17.6455 - val_loss: 12.9945 - val_mse: 12.9945\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 15.9432 - mse: 15.9432 - val_loss: 11.5183 - val_mse: 11.5183\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 14.4020 - mse: 14.4020 - val_loss: 10.1804 - val_mse: 10.1804\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 12.9730 - mse: 12.9730 - val_loss: 8.9353 - val_mse: 8.9353\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 11.6180 - mse: 11.6180 - val_loss: 7.7660 - val_mse: 7.7660\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 10.2894 - mse: 10.2894 - val_loss: 6.6676 - val_mse: 6.6676\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 9.0490 - mse: 9.0490 - val_loss: 5.6466 - val_mse: 5.6466\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 7.9118 - mse: 7.9118 - val_loss: 4.7148 - val_mse: 4.7148\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 6.8150 - mse: 6.8150 - val_loss: 3.8750 - val_mse: 3.8750\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 5.8148 - mse: 5.8148 - val_loss: 3.1459 - val_mse: 3.1459\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 4.8902 - mse: 4.8902 - val_loss: 2.5444 - val_mse: 2.5444\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 4.0757 - mse: 4.0757 - val_loss: 2.0853 - val_mse: 2.0853\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.3973 - mse: 3.3973 - val_loss: 1.7801 - val_mse: 1.7801\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.8348 - mse: 2.8348 - val_loss: 1.6242 - val_mse: 1.6242\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 2.4399 - mse: 2.4399 - val_loss: 1.5933 - val_mse: 1.5933\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 2.1853 - mse: 2.1853 - val_loss: 1.6449 - val_mse: 1.6449\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 2.0227 - mse: 2.0227 - val_loss: 1.7349 - val_mse: 1.7349\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.9266 - mse: 1.9266 - val_loss: 1.8177 - val_mse: 1.8177\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.8428 - mse: 1.8428 - val_loss: 1.8612 - val_mse: 1.8612\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.7857 - mse: 1.7857 - val_loss: 1.9129 - val_mse: 1.9129\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.7366 - mse: 1.7366 - val_loss: 1.9890 - val_mse: 1.9890\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.6898 - mse: 1.6898 - val_loss: 2.0477 - val_mse: 2.0477\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.6606 - mse: 1.6606 - val_loss: 2.0974 - val_mse: 2.0974\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.6316 - mse: 1.6316 - val_loss: 2.1243 - val_mse: 2.1243\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.6021 - mse: 1.6021 - val_loss: 2.1166 - val_mse: 2.1166\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.5785 - mse: 1.5785 - val_loss: 2.1375 - val_mse: 2.1375\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.5569 - mse: 1.5569 - val_loss: 2.1423 - val_mse: 2.1423\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.5427 - mse: 1.5427 - val_loss: 2.1699 - val_mse: 2.1699\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.5283 - mse: 1.5283 - val_loss: 2.2118 - val_mse: 2.2118\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.5270 - mse: 1.5270 - val_loss: 2.2656 - val_mse: 2.2656\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.5149 - mse: 1.5149 - val_loss: 2.3019 - val_mse: 2.3019\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.5053 - mse: 1.5053 - val_loss: 2.3105 - val_mse: 2.3105\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.4912 - mse: 1.4912 - val_loss: 2.2991 - val_mse: 2.2991\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4767 - mse: 1.4767 - val_loss: 2.3037 - val_mse: 2.3037\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4663 - mse: 1.4663 - val_loss: 2.3254 - val_mse: 2.3254\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.4537 - mse: 1.4537 - val_loss: 2.3207 - val_mse: 2.3207\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4433 - mse: 1.4433 - val_loss: 2.3153 - val_mse: 2.3153\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.4334 - mse: 1.4334 - val_loss: 2.3341 - val_mse: 2.3341\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4278 - mse: 1.4278 - val_loss: 2.3670 - val_mse: 2.3670\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4208 - mse: 1.4208 - val_loss: 2.3816 - val_mse: 2.3816\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.4130 - mse: 1.4130 - val_loss: 2.3704 - val_mse: 2.3704\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.4042 - mse: 1.4042 - val_loss: 2.3444 - val_mse: 2.3444\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.3961 - mse: 1.3961 - val_loss: 2.3346 - val_mse: 2.3346\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3892 - mse: 1.3892 - val_loss: 2.3174 - val_mse: 2.3174\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3812 - mse: 1.3812 - val_loss: 2.2985 - val_mse: 2.2985\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3728 - mse: 1.3728 - val_loss: 2.2881 - val_mse: 2.2881\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3638 - mse: 1.3638 - val_loss: 2.2651 - val_mse: 2.2651\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3562 - mse: 1.3562 - val_loss: 2.1979 - val_mse: 2.1979\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3485 - mse: 1.3485 - val_loss: 2.1567 - val_mse: 2.1567\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.3362 - mse: 1.3362 - val_loss: 2.1441 - val_mse: 2.1441\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3285 - mse: 1.3285 - val_loss: 2.1192 - val_mse: 2.1192\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.3234 - mse: 1.3234 - val_loss: 2.1214 - val_mse: 2.1214\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3180 - mse: 1.3180 - val_loss: 2.1418 - val_mse: 2.1418\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3113 - mse: 1.3113 - val_loss: 2.1651 - val_mse: 2.1651\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3053 - mse: 1.3053 - val_loss: 2.1930 - val_mse: 2.1930\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.3016 - mse: 1.3016 - val_loss: 2.2295 - val_mse: 2.2295\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3012 - mse: 1.3012 - val_loss: 2.2236 - val_mse: 2.2236\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.3051 - mse: 1.3051 - val_loss: 2.1602 - val_mse: 2.1602\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2954 - mse: 1.2954 - val_loss: 2.1448 - val_mse: 2.1448\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2982 - mse: 1.2982 - val_loss: 2.1317 - val_mse: 2.1317\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2897 - mse: 1.2897 - val_loss: 2.0923 - val_mse: 2.0923\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.2749 - mse: 1.2749 - val_loss: 2.0561 - val_mse: 2.0561\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2612 - mse: 1.2612 - val_loss: 2.0442 - val_mse: 2.0442\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.2472 - mse: 1.2472 - val_loss: 2.0453 - val_mse: 2.0453\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.2371 - mse: 1.2371 - val_loss: 2.0388 - val_mse: 2.0388\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.2254 - mse: 1.2254 - val_loss: 2.0307 - val_mse: 2.0307\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.2211 - mse: 1.2211 - val_loss: 2.0296 - val_mse: 2.0296\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2298 - mse: 1.2298 - val_loss: 2.0595 - val_mse: 2.0595\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.2352 - mse: 1.2352 - val_loss: 2.0851 - val_mse: 2.0851\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.2521 - mse: 1.2521 - val_loss: 2.1184 - val_mse: 2.1184\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.2690 - mse: 1.2690 - val_loss: 2.1742 - val_mse: 2.1742\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.2586 - mse: 1.2586 - val_loss: 2.2209 - val_mse: 2.2209\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.2356 - mse: 1.2356 - val_loss: 2.2762 - val_mse: 2.2762\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2080 - mse: 1.2080 - val_loss: 2.3257 - val_mse: 2.3257\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.1937 - mse: 1.1937 - val_loss: 2.3787 - val_mse: 2.3787\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1961 - mse: 1.1961 - val_loss: 2.4324 - val_mse: 2.4324\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.1944 - mse: 1.1944 - val_loss: 2.4230 - val_mse: 2.4230\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.1867 - mse: 1.1867 - val_loss: 2.3475 - val_mse: 2.3475\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1765 - mse: 1.1765 - val_loss: 2.3180 - val_mse: 2.3180\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.1651 - mse: 1.1651 - val_loss: 2.2452 - val_mse: 2.2452\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1501 - mse: 1.1501 - val_loss: 2.1378 - val_mse: 2.1378\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1371 - mse: 1.1371 - val_loss: 2.0858 - val_mse: 2.0858\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1372 - mse: 1.1372 - val_loss: 2.0488 - val_mse: 2.0488\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1498 - mse: 1.1498 - val_loss: 2.0378 - val_mse: 2.0378\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1477 - mse: 1.1477 - val_loss: 2.0763 - val_mse: 2.0763\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1322 - mse: 1.1322 - val_loss: 2.1365 - val_mse: 2.1365\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1205 - mse: 1.1205 - val_loss: 2.2026 - val_mse: 2.2026\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1203 - mse: 1.1203 - val_loss: 2.2119 - val_mse: 2.2119\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.1090 - mse: 1.1090 - val_loss: 2.1794 - val_mse: 2.1794\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1016 - mse: 1.1016 - val_loss: 2.1179 - val_mse: 2.1179\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.0949 - mse: 1.0949 - val_loss: 2.0604 - val_mse: 2.0604\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.0926 - mse: 1.0926 - val_loss: 1.9794 - val_mse: 1.9794\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.0990 - mse: 1.0990 - val_loss: 1.9144 - val_mse: 1.9144\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0952 - mse: 1.0952 - val_loss: 1.9033 - val_mse: 1.9033\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.0850 - mse: 1.0850 - val_loss: 1.9339 - val_mse: 1.9339\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.0667 - mse: 1.0667 - val_loss: 1.9800 - val_mse: 1.9800\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0699 - mse: 1.0699 - val_loss: 2.0305 - val_mse: 2.0305\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.0723 - mse: 1.0723 - val_loss: 2.0597 - val_mse: 2.0597\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.0749 - mse: 1.0749 - val_loss: 2.1010 - val_mse: 2.1010\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.0788 - mse: 1.0788 - val_loss: 2.1229 - val_mse: 2.1229\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.5335 - mse: 2.5335\n",
            "Mean Squared Error (TensorFlow/Keras): 2.5335023403167725\n",
            "-----------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "3/3 [==============================] - 1s 91ms/step - loss: 0.7682 - accuracy: 0.1765 - val_loss: 0.7368 - val_accuracy: 0.2941\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6925 - accuracy: 0.5000 - val_loss: 0.6592 - val_accuracy: 0.7059\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6353 - accuracy: 0.7206 - val_loss: 0.5894 - val_accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5828 - accuracy: 0.9118 - val_loss: 0.5291 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5390 - accuracy: 0.9118 - val_loss: 0.4772 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.5030 - accuracy: 0.9118 - val_loss: 0.4300 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4703 - accuracy: 0.9118 - val_loss: 0.3870 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4398 - accuracy: 0.9118 - val_loss: 0.3488 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4129 - accuracy: 0.9118 - val_loss: 0.3146 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3887 - accuracy: 0.9118 - val_loss: 0.2829 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3686 - accuracy: 0.9118 - val_loss: 0.2547 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3500 - accuracy: 0.9118 - val_loss: 0.2313 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.3369 - accuracy: 0.9118 - val_loss: 0.2114 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3227 - accuracy: 0.9118 - val_loss: 0.1953 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.3117 - accuracy: 0.9118 - val_loss: 0.1806 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3014 - accuracy: 0.9118 - val_loss: 0.1694 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2921 - accuracy: 0.9118 - val_loss: 0.1601 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2848 - accuracy: 0.9118 - val_loss: 0.1504 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2776 - accuracy: 0.9118 - val_loss: 0.1414 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2717 - accuracy: 0.9118 - val_loss: 0.1340 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2657 - accuracy: 0.9118 - val_loss: 0.1303 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2609 - accuracy: 0.9118 - val_loss: 0.1261 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2559 - accuracy: 0.9118 - val_loss: 0.1227 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.2524 - accuracy: 0.9118 - val_loss: 0.1187 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2483 - accuracy: 0.9118 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2448 - accuracy: 0.9118 - val_loss: 0.1130 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2417 - accuracy: 0.9118 - val_loss: 0.1122 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2381 - accuracy: 0.9118 - val_loss: 0.1147 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2351 - accuracy: 0.9118 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2331 - accuracy: 0.9118 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2307 - accuracy: 0.9118 - val_loss: 0.1144 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2282 - accuracy: 0.9118 - val_loss: 0.1144 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2261 - accuracy: 0.9118 - val_loss: 0.1137 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2243 - accuracy: 0.9118 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2222 - accuracy: 0.9118 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2205 - accuracy: 0.9118 - val_loss: 0.1060 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.2194 - accuracy: 0.9118 - val_loss: 0.1028 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2181 - accuracy: 0.9118 - val_loss: 0.0997 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2170 - accuracy: 0.9118 - val_loss: 0.0966 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2161 - accuracy: 0.9118 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2155 - accuracy: 0.9118 - val_loss: 0.0927 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2147 - accuracy: 0.9118 - val_loss: 0.0924 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2129 - accuracy: 0.9118 - val_loss: 0.0938 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2119 - accuracy: 0.9118 - val_loss: 0.0947 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2100 - accuracy: 0.9118 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2094 - accuracy: 0.9118 - val_loss: 0.0974 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2076 - accuracy: 0.9118 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2065 - accuracy: 0.9118 - val_loss: 0.0965 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2054 - accuracy: 0.9118 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2039 - accuracy: 0.9118 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2028 - accuracy: 0.9118 - val_loss: 0.0983 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2013 - accuracy: 0.9118 - val_loss: 0.1007 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2003 - accuracy: 0.9118 - val_loss: 0.1034 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1987 - accuracy: 0.9118 - val_loss: 0.1042 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1975 - accuracy: 0.9118 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1961 - accuracy: 0.9118 - val_loss: 0.1030 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1947 - accuracy: 0.9118 - val_loss: 0.1019 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1934 - accuracy: 0.9118 - val_loss: 0.1015 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1922 - accuracy: 0.9118 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1911 - accuracy: 0.9118 - val_loss: 0.1043 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1897 - accuracy: 0.9118 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1884 - accuracy: 0.9118 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1872 - accuracy: 0.9118 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1863 - accuracy: 0.9118 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1852 - accuracy: 0.9118 - val_loss: 0.0957 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1847 - accuracy: 0.9118 - val_loss: 0.0924 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1845 - accuracy: 0.9118 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1831 - accuracy: 0.9118 - val_loss: 0.0927 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1816 - accuracy: 0.9118 - val_loss: 0.0947 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1801 - accuracy: 0.9118 - val_loss: 0.0966 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1792 - accuracy: 0.9118 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1775 - accuracy: 0.9118 - val_loss: 0.1001 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1766 - accuracy: 0.9118 - val_loss: 0.1002 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1755 - accuracy: 0.9265 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1741 - accuracy: 0.9265 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1734 - accuracy: 0.9265 - val_loss: 0.1073 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1709 - accuracy: 0.9265 - val_loss: 0.1145 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1707 - accuracy: 0.9265 - val_loss: 0.1212 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1692 - accuracy: 0.9265 - val_loss: 0.1247 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1678 - accuracy: 0.9265 - val_loss: 0.1265 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1667 - accuracy: 0.9265 - val_loss: 0.1274 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1658 - accuracy: 0.9265 - val_loss: 0.1287 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1642 - accuracy: 0.9265 - val_loss: 0.1331 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1630 - accuracy: 0.9265 - val_loss: 0.1349 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.1621 - accuracy: 0.9265 - val_loss: 0.1337 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1601 - accuracy: 0.9265 - val_loss: 0.1264 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1587 - accuracy: 0.9265 - val_loss: 0.1179 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1582 - accuracy: 0.9265 - val_loss: 0.1101 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1580 - accuracy: 0.9265 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1580 - accuracy: 0.9265 - val_loss: 0.1005 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1570 - accuracy: 0.9265 - val_loss: 0.0999 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1560 - accuracy: 0.9265 - val_loss: 0.0981 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1557 - accuracy: 0.9265 - val_loss: 0.0962 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1559 - accuracy: 0.9265 - val_loss: 0.0955 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1541 - accuracy: 0.9265 - val_loss: 0.0977 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1531 - accuracy: 0.9265 - val_loss: 0.1004 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1514 - accuracy: 0.9265 - val_loss: 0.1037 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1495 - accuracy: 0.9265 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1471 - accuracy: 0.9265 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1463 - accuracy: 0.9265 - val_loss: 0.1271 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5917 - accuracy: 0.8636\n",
            "Test Loss (TensorFlow/Keras): 0.5916541218757629\n",
            "Test Accuracy (TensorFlow/Keras): 0.8636363744735718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define the model\n",
        "class PolyRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(PolyRegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_poly_train_tensor = torch.tensor(X_poly_train_scaled, dtype=torch.float32)\n",
        "y_poly_train_tensor = torch.tensor(y_poly_train.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "X_poly_test_tensor = torch.tensor(X_poly_test_scaled, dtype=torch.float32)\n",
        "y_poly_test_tensor = torch.tensor(y_poly_test.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_poly_train_tensor, y_poly_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "model_pytorch = PolyRegressionModel(X_poly_train_scaled.shape[1])\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_pytorch(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    outputs = model_pytorch(X_poly_test_tensor)\n",
        "    mse_pytorch = criterion(outputs, y_poly_test_tensor)\n",
        "    print(\"Mean Squared Error (PyTorch):\", mse_pytorch.item())\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "# Implement classification\n",
        "# Define the model for classification in PyTorch\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model_pytorch_classification = ClassificationModel(X_class_train_scaled.shape[1])\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors for classification\n",
        "X_class_train_tensor = torch.tensor(X_class_train_scaled, dtype=torch.float32)\n",
        "y_class_train_tensor = torch.tensor(y_class_train, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "X_class_test_tensor = torch.tensor(X_class_test_scaled, dtype=torch.float32)\n",
        "y_class_test_tensor = torch.tensor(y_class_test, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Create DataLoader for classification\n",
        "train_data_classification = TensorDataset(X_class_train_tensor, y_class_train_tensor)\n",
        "train_loader_classification = DataLoader(train_data_classification, batch_size=32, shuffle=True)\n",
        "\n",
        "test_data_classification = TensorDataset(X_class_test_tensor, y_class_test_tensor)\n",
        "test_loader_classification = DataLoader(test_data_classification, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion_classification = nn.BCELoss()\n",
        "optimizer_classification = optim.Adam(model_pytorch_classification.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_classification:\n",
        "        optimizer_classification.zero_grad()\n",
        "        outputs = model_pytorch_classification(inputs)\n",
        "        loss_classification = criterion_classification(outputs, labels)\n",
        "        loss_classification.backward()\n",
        "        optimizer_classification.step()\n",
        "        running_loss += loss_classification.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
        "\n",
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    outputs_classification = model_pytorch_classification(X_class_test_tensor)\n",
        "    loss_classification = criterion_classification(outputs_classification, y_class_test_tensor)\n",
        "    predictions = (outputs_classification > 0.5).float()\n",
        "    accuracy_pytorch_classification = torch.mean((predictions == y_class_test_tensor).float())\n",
        "    print(\"Test Loss (PyTorch):\", loss_classification.item())\n",
        "    print(\"Test Accuracy (PyTorch):\", accuracy_pytorch_classification.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9OgYFzt0uFZ",
        "outputId": "93edbb26-9811-4091-fb2c-2b44f9dbcbde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 50.64205455780029\n",
            "Epoch 2, Loss: 46.37148857116699\n",
            "Epoch 3, Loss: 43.307146072387695\n",
            "Epoch 4, Loss: 40.848365783691406\n",
            "Epoch 5, Loss: 37.393853187561035\n",
            "Epoch 6, Loss: 34.89534282684326\n",
            "Epoch 7, Loss: 30.88826274871826\n",
            "Epoch 8, Loss: 28.516469478607178\n",
            "Epoch 9, Loss: 24.55151653289795\n",
            "Epoch 10, Loss: 21.053458213806152\n",
            "Epoch 11, Loss: 17.380541801452637\n",
            "Epoch 12, Loss: 14.035048007965088\n",
            "Epoch 13, Loss: 11.5358145236969\n",
            "Epoch 14, Loss: 8.92451524734497\n",
            "Epoch 15, Loss: 7.464125394821167\n",
            "Epoch 16, Loss: 6.47645890712738\n",
            "Epoch 17, Loss: 5.9928401708602905\n",
            "Epoch 18, Loss: 5.293258428573608\n",
            "Epoch 19, Loss: 5.852438926696777\n",
            "Epoch 20, Loss: 5.865965247154236\n",
            "Epoch 21, Loss: 5.552149474620819\n",
            "Epoch 22, Loss: 5.563105821609497\n",
            "Epoch 23, Loss: 5.352930426597595\n",
            "Epoch 24, Loss: 5.192534565925598\n",
            "Epoch 25, Loss: 5.061388969421387\n",
            "Epoch 26, Loss: 5.036796927452087\n",
            "Epoch 27, Loss: 4.667187690734863\n",
            "Epoch 28, Loss: 4.7242395877838135\n",
            "Epoch 29, Loss: 4.399576961994171\n",
            "Epoch 30, Loss: 4.45056414604187\n",
            "Epoch 31, Loss: 4.727268099784851\n",
            "Epoch 32, Loss: 4.629208207130432\n",
            "Epoch 33, Loss: 4.466622591018677\n",
            "Epoch 34, Loss: 4.6051201820373535\n",
            "Epoch 35, Loss: 4.390138626098633\n",
            "Epoch 36, Loss: 4.262747645378113\n",
            "Epoch 37, Loss: 4.284865021705627\n",
            "Epoch 38, Loss: 4.217891216278076\n",
            "Epoch 39, Loss: 4.402435064315796\n",
            "Epoch 40, Loss: 4.070083320140839\n",
            "Epoch 41, Loss: 4.0790499448776245\n",
            "Epoch 42, Loss: 4.158356308937073\n",
            "Epoch 43, Loss: 4.2567527294158936\n",
            "Epoch 44, Loss: 4.1320706605911255\n",
            "Epoch 45, Loss: 4.069117784500122\n",
            "Epoch 46, Loss: 4.108634829521179\n",
            "Epoch 47, Loss: 4.0387184619903564\n",
            "Epoch 48, Loss: 4.085042119026184\n",
            "Epoch 49, Loss: 4.0279489159584045\n",
            "Epoch 50, Loss: 4.081250309944153\n",
            "Epoch 51, Loss: 4.1561585664749146\n",
            "Epoch 52, Loss: 3.8823680877685547\n",
            "Epoch 53, Loss: 4.006797790527344\n",
            "Epoch 54, Loss: 3.868429183959961\n",
            "Epoch 55, Loss: 3.857464909553528\n",
            "Epoch 56, Loss: 3.785340428352356\n",
            "Epoch 57, Loss: 3.778611660003662\n",
            "Epoch 58, Loss: 3.917127013206482\n",
            "Epoch 59, Loss: 3.787052035331726\n",
            "Epoch 60, Loss: 3.777353882789612\n",
            "Epoch 61, Loss: 3.721151649951935\n",
            "Epoch 62, Loss: 3.790489673614502\n",
            "Epoch 63, Loss: 3.8411009311676025\n",
            "Epoch 64, Loss: 3.8711787462234497\n",
            "Epoch 65, Loss: 3.781912684440613\n",
            "Epoch 66, Loss: 3.6343942284584045\n",
            "Epoch 67, Loss: 3.814574897289276\n",
            "Epoch 68, Loss: 3.6559374928474426\n",
            "Epoch 69, Loss: 3.7901651859283447\n",
            "Epoch 70, Loss: 3.8007360696792603\n",
            "Epoch 71, Loss: 3.6799871921539307\n",
            "Epoch 72, Loss: 4.00602525472641\n",
            "Epoch 73, Loss: 3.8249335289001465\n",
            "Epoch 74, Loss: 3.520590364933014\n",
            "Epoch 75, Loss: 3.6663355827331543\n",
            "Epoch 76, Loss: 3.4950226545333862\n",
            "Epoch 77, Loss: 3.6075485944747925\n",
            "Epoch 78, Loss: 3.506382167339325\n",
            "Epoch 79, Loss: 3.6194695234298706\n",
            "Epoch 80, Loss: 3.584289073944092\n",
            "Epoch 81, Loss: 3.669167995452881\n",
            "Epoch 82, Loss: 3.5929179191589355\n",
            "Epoch 83, Loss: 3.7629210352897644\n",
            "Epoch 84, Loss: 3.5286757946014404\n",
            "Epoch 85, Loss: 3.5828717947006226\n",
            "Epoch 86, Loss: 3.8005247712135315\n",
            "Epoch 87, Loss: 3.515543222427368\n",
            "Epoch 88, Loss: 3.6912617683410645\n",
            "Epoch 89, Loss: 3.769680619239807\n",
            "Epoch 90, Loss: 3.4951605796813965\n",
            "Epoch 91, Loss: 3.4974106550216675\n",
            "Epoch 92, Loss: 3.409173846244812\n",
            "Epoch 93, Loss: 3.5011380910873413\n",
            "Epoch 94, Loss: 3.5703965425491333\n",
            "Epoch 95, Loss: 3.6235410571098328\n",
            "Epoch 96, Loss: 3.3874996304512024\n",
            "Epoch 97, Loss: 3.433497190475464\n",
            "Epoch 98, Loss: 3.4394460916519165\n",
            "Epoch 99, Loss: 3.512502908706665\n",
            "Epoch 100, Loss: 3.4720852971076965\n",
            "Mean Squared Error (PyTorch): 1.9616214036941528\n",
            "-----------------------------------------------------------------------\n",
            "Epoch 1, Loss: 1.9761527180671692\n",
            "Epoch 2, Loss: 1.8717404007911682\n",
            "Epoch 3, Loss: 1.7546031475067139\n",
            "Epoch 4, Loss: 1.6469786167144775\n",
            "Epoch 5, Loss: 1.547632783651352\n",
            "Epoch 6, Loss: 1.4584133327007294\n",
            "Epoch 7, Loss: 1.3542245924472809\n",
            "Epoch 8, Loss: 1.2787952423095703\n",
            "Epoch 9, Loss: 1.1870254874229431\n",
            "Epoch 10, Loss: 1.1188945174217224\n",
            "Epoch 11, Loss: 1.0136100351810455\n",
            "Epoch 12, Loss: 0.9429911077022552\n",
            "Epoch 13, Loss: 0.9136154651641846\n",
            "Epoch 14, Loss: 0.8647016882896423\n",
            "Epoch 15, Loss: 0.8897895663976669\n",
            "Epoch 16, Loss: 0.7822344452142715\n",
            "Epoch 17, Loss: 0.811777725815773\n",
            "Epoch 18, Loss: 0.7519679218530655\n",
            "Epoch 19, Loss: 0.6994471102952957\n",
            "Epoch 20, Loss: 0.7637758404016495\n",
            "Epoch 21, Loss: 0.6708725392818451\n",
            "Epoch 22, Loss: 0.6587114185094833\n",
            "Epoch 23, Loss: 0.7574941664934158\n",
            "Epoch 24, Loss: 0.6751681268215179\n",
            "Epoch 25, Loss: 0.6269494816660881\n",
            "Epoch 26, Loss: 0.6532379388809204\n",
            "Epoch 27, Loss: 0.7286616414785385\n",
            "Epoch 28, Loss: 0.6125232577323914\n",
            "Epoch 29, Loss: 0.6337805390357971\n",
            "Epoch 30, Loss: 0.6963255703449249\n",
            "Epoch 31, Loss: 0.5922800451517105\n",
            "Epoch 32, Loss: 0.623800128698349\n",
            "Epoch 33, Loss: 0.5809638127684593\n",
            "Epoch 34, Loss: 0.6173737794160843\n",
            "Epoch 35, Loss: 0.6455591917037964\n",
            "Epoch 36, Loss: 0.5992214381694794\n",
            "Epoch 37, Loss: 0.6257091909646988\n",
            "Epoch 38, Loss: 0.5963110625743866\n",
            "Epoch 39, Loss: 0.5916134715080261\n",
            "Epoch 40, Loss: 0.6089556664228439\n",
            "Epoch 41, Loss: 0.6230863854289055\n",
            "Epoch 42, Loss: 0.5631893575191498\n",
            "Epoch 43, Loss: 0.5634665042161942\n",
            "Epoch 44, Loss: 0.5972015857696533\n",
            "Epoch 45, Loss: 0.5943616181612015\n",
            "Epoch 46, Loss: 0.6080821603536606\n",
            "Epoch 47, Loss: 0.6188044175505638\n",
            "Epoch 48, Loss: 0.5482544600963593\n",
            "Epoch 49, Loss: 0.6146435290575027\n",
            "Epoch 50, Loss: 0.5881862491369247\n",
            "Epoch 51, Loss: 0.5139707252383232\n",
            "Epoch 52, Loss: 0.5397045463323593\n",
            "Epoch 53, Loss: 0.5364416837692261\n",
            "Epoch 54, Loss: 0.5517123937606812\n",
            "Epoch 55, Loss: 0.4962914064526558\n",
            "Epoch 56, Loss: 0.5459488108754158\n",
            "Epoch 57, Loss: 0.5453644767403603\n",
            "Epoch 58, Loss: 0.4864468649029732\n",
            "Epoch 59, Loss: 0.5149829983711243\n",
            "Epoch 60, Loss: 0.5303758233785629\n",
            "Epoch 61, Loss: 0.5730704367160797\n",
            "Epoch 62, Loss: 0.5215875431895256\n",
            "Epoch 63, Loss: 0.5336358845233917\n",
            "Epoch 64, Loss: 0.49205195903778076\n",
            "Epoch 65, Loss: 0.4597942680120468\n",
            "Epoch 66, Loss: 0.4729594513773918\n",
            "Epoch 67, Loss: 0.48566532135009766\n",
            "Epoch 68, Loss: 0.5478964522480965\n",
            "Epoch 69, Loss: 0.48076366633176804\n",
            "Epoch 70, Loss: 0.49281851947307587\n",
            "Epoch 71, Loss: 0.48793943226337433\n",
            "Epoch 72, Loss: 0.4717681109905243\n",
            "Epoch 73, Loss: 0.46211913973093033\n",
            "Epoch 74, Loss: 0.4355948306620121\n",
            "Epoch 75, Loss: 0.4938365966081619\n",
            "Epoch 76, Loss: 0.5085481107234955\n",
            "Epoch 77, Loss: 0.4506707191467285\n",
            "Epoch 78, Loss: 0.44388270378112793\n",
            "Epoch 79, Loss: 0.47943249344825745\n",
            "Epoch 80, Loss: 0.4693519026041031\n",
            "Epoch 81, Loss: 0.45879368484020233\n",
            "Epoch 82, Loss: 0.44193127006292343\n",
            "Epoch 83, Loss: 0.461196206510067\n",
            "Epoch 84, Loss: 0.4245002046227455\n",
            "Epoch 85, Loss: 0.4263984113931656\n",
            "Epoch 86, Loss: 0.40732038021087646\n",
            "Epoch 87, Loss: 0.43749822676181793\n",
            "Epoch 88, Loss: 0.41729532182216644\n",
            "Epoch 89, Loss: 0.4223858490586281\n",
            "Epoch 90, Loss: 0.4269239455461502\n",
            "Epoch 91, Loss: 0.3986727297306061\n",
            "Epoch 92, Loss: 0.39725225418806076\n",
            "Epoch 93, Loss: 0.4349762052297592\n",
            "Epoch 94, Loss: 0.3875085860490799\n",
            "Epoch 95, Loss: 0.4092695564031601\n",
            "Epoch 96, Loss: 0.40092355385422707\n",
            "Epoch 97, Loss: 0.38186264783143997\n",
            "Epoch 98, Loss: 0.38505910336971283\n",
            "Epoch 99, Loss: 0.3974142447113991\n",
            "Epoch 100, Loss: 0.37988586723804474\n",
            "Test Loss (PyTorch): 0.5991433262825012\n",
            "Test Accuracy (PyTorch): 0.8636363744735718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#Conclusion from training two frameworks on the provided dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ihJear9DNMDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data compare: TenserFlow"
      ],
      "metadata": {
        "id": "P17pPabaT0Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Epoch 1/100\n",
        "3/3 [==============================] - 2s 231ms/step - loss: 17.6455 - mse: 17.6455 - val_loss: 12.9945 - val_mse: 12.9945\n",
        "...\n",
        "Epoch 100/100\n",
        "3/3 [==============================] - 0s 15ms/step - loss: 1.0788 - mse: 1.0788 - val_loss: 2.1229 - val_mse: 2.1229\n",
        "1/1 [==============================] - 0s 23ms/step - loss: 2.5335 - mse: 2.5335\n",
        "Mean Squared Error (TensorFlow/Keras): 2.5335023403167725\n",
        "-----------------------------------------------------------------------\n",
        "Epoch 1/100\n",
        "3/3 [==============================] - 1s 91ms/step - loss: 0.7682 - accuracy: 0.1765 - val_loss: 0.7368 - val_accuracy: 0.2941\n",
        "...\n",
        "Epoch 100/100\n",
        "3/3 [==============================] - 0s 19ms/step - loss: 0.1463 - accuracy: 0.9265 - val_loss: 0.1271 - val_accuracy: 1.0000\n",
        "1/1 [==============================] - 0s 34ms/step - loss: 0.5917 - accuracy: 0.8636\n",
        "Test Loss (TensorFlow/Keras): 0.5916541218757629\n",
        "Test Accuracy (TensorFlow/Keras): 0.8636363744735718\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gniTHohuUtYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data compare: PyTorch"
      ],
      "metadata": {
        "id": "frrRRPPuUdNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Epoch 1, Loss: 50.64205455780029\n",
        "...\n",
        "Epoch 100, Loss: 3.4720852971076965\n",
        "Mean Squared Error (PyTorch): 1.9616214036941528\n",
        "-----------------------------------------------------------------------\n",
        "Epoch 1, Loss: 1.9761527180671692\n",
        "...\n",
        "Epoch 100, Loss: 0.37988586723804474\n",
        "Test Loss (PyTorch): 0.5991433262825012\n",
        "Test Accuracy (PyTorch): 0.8636363744735718\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "edzBuJ2yU1Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Based on the results provided, both frameworks, PyTorch and TensorFlow, were used to train and evaluate a deep neural network model. The training process involved multiple epochs, with the loss decreasing over time, indicating that the model was learning from the data. In the case of PyTorch, the mean squared error decreased from approximately 50.64 to 3.47, while for TensorFlow, the loss decreased from around 17.6455 to 1.0788. Additionally, the test accuracy for both PyTorch and TensorFlow was reported to be approximately 86.36%, indicating that the models performed well on unseen data. These results suggest that both frameworks were effective in training the model, but PyTorch showcased slightly better performance in terms of convergence speed."
      ],
      "metadata": {
        "id": "giLnXnKFNDyg"
      }
    }
  ]
}